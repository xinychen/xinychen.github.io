<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a href="index.html">About Me</a></li>
            <li><a class="active" href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
          </ul>
  <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">
<br>
<br>
<br>
<br>

<h1>News</h1>
<hr>
<br>
<ul>
  <ul>
    <li>
      <p> 29 Apr 2021: Our paper <a href="https://arxiv.org/abs/1905.08645">"Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://ieeeittrans.ee.technion.ac.il/"> IEEE Transactions on Information Theory. </a> </p>
    </li>
    <li>
      <p> 22 Jan 2021: Two papers were accepted to <b>AISTATS 2021</b> (24th International Conference on
Artificial Intelligence and Statistics):</p>
      <p><a href="https://arxiv.org/abs/2002.10542">Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence </a><br>
        joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien. </p>
    <p><a href="https://arxiv.org/abs/2006.10311">SGD for structured nonconvex functions: Learning rates, minibatching and interpolation </a> <br>
        joint work with Robert M. Gower and Othmane Sebbouh.</p>
    </li>
    <li>
      <p> 16 Oct 2020: I am delighted to be selected as the runner-up of the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/"> OR Society’s Doctoral Award </a> for 2019.
This is an award for the "Most Distinguished Body of Research leading to the Award of a Doctorate in the field of Operational Research" in the United Kingdom.
</li>
    <li>
      <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a> (<b>SISC</b>) </p>
    </li>
    <li>
      <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods
"</a>, joint work with Peter Richtarik,
        was accepted to
<a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a> (<b>COAP</b>) </p>
    </li>
    <li>
      <p> 20 Jun 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization
</a>, joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower and Peter Richtárik. </p>
    </li>
    <li>
      <p> 18 Jun 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2006.10311"> SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation</a>, joint work with Robert M. Gower and Othmane Sebbouh. </p>
    </li>
    <li>
      <p> 01 Jun 2020: Two papers were accepted to <b>ICML 2020</b> (37th
        International Conference on Machine Learning):</p>
      <p><a href="https://arxiv.org/abs/2003.10422">A Unified
          Theory of Decentralized SGD with Changing Topology and
          Local Updates </a><br>
        joint work with Anastasia Koloskova, Sadra Boreiri, Martin
        Jaggi and Sebastian U. Stich. </p>
    <p><a href="https://arxiv.org/abs/2007.04202">Stochastic Hamiltonian Gradient Methods for Smooth
    Games </a> <br>
        joint work with Hugo Berard, Alexia Jolicoeur-Martineau,
        Pascal Vincent, Simon Lacoste-Julien and Ioannis
        Mitliagkas.</p>
    </li>
    <li>
      <p> 23 Mar 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2003.10422"> A Unified
          Theory of Decentralized SGD with Changing Topology and
          Local Updates</a>, joint work with Anastasia Koloskova,
        Sadra Boreiri, Martin Jaggi and Sebastian U. Stich. </p>
    </li>
    <li>
      <p> 24 Feb 2020: <b>New Paper out:</b> <a
          href="https://arxiv.org/abs/2002.10542"> Stochastic
          Polyak Step-size for SGD: An Adaptive Learning Rate for
          Fast Convergence</a>, joint work with Sharan Vaswani,
        Issam Laradji and Simon Lacoste-Julien. </p>
    </li>
    <li>
      <p> 19 December 2019: I am delighted to be awarded the <a
          href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
          IVADO Fellow Postdoctoral Scholarship </a>. <br>
        For more details on The Institute for Data Valorisation
        (IVADO) and its mission check out the <a
          href="https://ivado.ca/en/about-us/"> IVADO's website </a>
      </p>
    </li>
    <li>
      <p> 01 September 2019: Officially started as a Postdoctoral
        Fellow at <a href="https://mila.quebec/en/"> Mila -
          Quebec Artificial Intelligence Institute</a>. </p>
    </li>
    <li>
      <p> 28 June 2019: <b>Thesis Defense!</b> <br>
        I have successfully defended my PhD thesis "Randomized
        Iterative Methods for Linear Systems: Momentum,
        Inexactness and Gossip" today. A copy of the final version
        of the manuscript is available <a
          href="PhDThesis_Loizou2019.pdf">here</a> </p>
    </li>
    <li>
      <p> 20 May 2019: <b>New Paper out: </b> <a
          href="RandGossip.pdf"> Revisiting Randomized Gossip
          Algorithms: General Framework, Convergence Rates and
          Novel Block and Accelerated Protocols</a>. joint work
        with Peter Richtarik </p>
    </li>
    <li>
      <p> 22 Apr 2019: Two papers accepted to ICML 2019 (36th
        International Conference on Machine Learning):</p>
      <p><a href="https://arxiv.org/abs/1901.09401">SGD: General
          Analysis and Improved Rates</a>. joint work with Robert
        Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
        and Peter Richtarik. </p>
      <p><a href="https://arxiv.org/abs/1811.10792">Stochastic
          Gradient Push for Distributed Deep Learning</a>. joint
        work with Mahmoud Assran, Nicolas Ballas and Michael
        Rabbat </p>
    </li>
    <li>
      <p> 07 Apr - 14 May 2019: I am visiting <a
          href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
          Peter Richtarik</a> at <a
          href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
      </p>
    </li>
    <li>
      <p> 19 Mar 2019: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1903.07971"> Convergence
          Analysis of Inexact Randomized Iterative Methods</a>.
        joint work with Peter Richtarik. </p>
    </li>
    <li>
      <p> 01 Feb 2019: The paper <a
          href="https://arxiv.org/abs/1810.13084"> Provably
          Accelerated Randomized Gossip Algorithms</a>, coauthored
        with Mike Rabbat and Peter Richtarik, was accepted to the
        44th International Conference on Acoustics, Speech, and
        Signal Processing (ICASSP 2019). </p>
    </li>
    <li>
      <p> 27 Jan 2019: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1901.09401"> SGD: General
          Analysis and Improved Rates</a>. joint work with Robert
        Mansel Gower, Xun Qian, Alibek Sailanbayev, Egor Shulgin
        and Peter Richtarik. </p>
    </li>
    <li>
      <p> 27 Nov 2018: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1811.10792"> Stochastic
          Gradient Push for Distributed Deep Learning</a>. joint
        work with Mahmoud Assran, Nicolas Ballas and Michael
        Rabbat </p>
    </li>
    <li>
      <p> 31 Oct 2018: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1810.13084"> Provably
          Accelerated Randomized Gossip Algorithms</a>. joint work
        with Michael Rabbat and Peter Richtarik </p>
    </li>
    <li>
      <p> 13 August - 17 Aug 2018: I am attending <a
          href="http://coral.ie.lehigh.edu/%7Emopta//">
          DIMACS/TRIPODS/MOPTA </a>, this week. On Tuesday I am
        presenting a poster and on Thursday I am giving a talk on
        ''Revisiting Randomized Gossip Algorithms". </p>
    </li>
    <li>
      <p> 09 Jul 2018: <b>New Paper out: </b> <a
          href="acc_gossip.pdf"> Accelerated Gossip via Stochastic
          Heavy Ball Method </a>. The paper is accepted to 56th
        Annual Allerton Conference on Communication, Control, and
        Computing, 2018 </p>
    </li>
    <li>
      <p> 01 Jul - 06 Jul 2018: I am attending <a
          href="https://ismp2018.sciencesconf.org/"> 23rd
          International Symposium on Mathematical Programming </a>
        this week. On Friday I am presenting our latest work:
        ''Convergence Analysis of Inexact Randomized Iterative
        Methods" </p>
    </li>
    <li>
      <p> 10 Apr - 15 May 2018: I am visiting <a
          href="http://mtakac.com/"> Martin Takac </a> and the <a
          href="http://optml.lehigh.edu/people/"> Optimization and
          Machine Learning Research Group at Lehigh University </a>
      </p>
    </li>
    <li>
      <p> 01 Feb - 21 Mar 2018: I am visiting <a
          href="http://www.maths.ed.ac.uk/%7Erichtarik/index.html">
          Peter Richtarik</a> at <a
          href="https://vcc.kaust.edu.sa/Pages/Home.aspx"> KAUST </a>
      </p>
    </li>
    <li>
      <p> 22 Dec 2017: <b>New Paper out: </b> <a
          href="https://arxiv.org/abs/1712.09677"> Momentum and
          Stochastic Momentum for Stochastic Gradient, Newton,
          Proximal Point and Subspace Descent Methods </a> </p>
    </li>
    <li>
      <p> 04-09 Dec 2017: I am attending <a
          href="https://nips.cc/Conferences/2017"> NIPS</a> this
        week. Our work <a href="https://arxiv.org/abs/1710.10737">
          Linearly convergent stochastic heavy ball method for
          minimizing generalization error </a> is presented at <a
          href="http://opt-ml.org/"> NIPS Workshop on Optimization
          for Machine Learning </a> </p>
    </li>
    <li>
      <p> 1-10 Oct 2017: I am visiting <a
          href="https://simons.berkeley.edu/"> Simons Institute
          for the Theory of Computing </a> at Berkeley,
        California. I am attending the workshop <a
          href="https://simons.berkeley.edu/workshops/optimization2017-2">
          Fast Iterative Methods in Optimization </a>. </p>
    </li>
  </ul>
</ul>

<br>
<br>
</div>

 </body>
</html>
