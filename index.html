<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Nicolas Loizou</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26204762-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  </head>


  <body>
    <div id="page">
      <div id="header_wrapper">
        <div id="header" class="main">
          <ul class="menu">
            <li><a class="active" href="index.html">About Me</a></li>
            <li><a href="news.html"> News </a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="bio.html">Bio</a></li>
            <li><a href="ProfServ.html">Professional Services</a></li>
            <li><a href="teaching.html">Teaching</a></li>
          </ul>
          <br>
          <script src="table_header.js"></script> </div>

      <div id="wrapper" class="main">
        <div id="content">

<br>
<br>
<br>
<br>
<br>
<b>Personal Update :</b><br> In January 2022, I will be joining the <a
    href="https://engineering.jhu.edu/">
    Whiting School of Engineering</a> at <a
        href="https://www.jhu.edu/">
        The Johns Hopkins University</a> as an Assistant Professor in the <a
            href="https://engineering.jhu.edu/ams/">
            Department of Applied
Mathematics and Statistics</a> (AMS) and the <a
    href="https://www.minds.jhu.edu/about-minds-2/">
    Mathematical Institute for Data Science </a> (MINDS) with a
secondary appointment in the <a
    href="https://www.cs.jhu.edu/">
    Department of Computer Science</a>!!!
    <br>
    <br>
<h1>About Me</h1>
<hr> <br>
I am an <a
    href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
    IVADO</a> postdoctoral fellow at <a href="https://mila.quebec/en/">
  Mila - Quebec Artificial Intelligence Institute</a> and <a href="https://diro.umontreal.ca/english/home/">
DIRO, UdeM</a>. I work closely with <a
  href="http://www.iro.umontreal.ca/%7Eslacoste/"> Dr. Simon
  Lacoste-Julien </a> and <a href="http://mitliagkas.github.io">
  Dr. Ioannis Mitliagkas</a>. My current research focuses on the
theory and applications of convex and non-convex optimization in
large-scale machine learning and data science problems.<br>
<br>
I obtained my PhD from <a href="http://www.ed.ac.uk/">The
University of Edinburgh</a>, <a href="http://www.maths.ed.ac.uk/">School of Mathematics</a>. More specifically I was a member of the <a
href="http://www.maths.ed.ac.uk/school-of-mathematics/research/ergo">Operational
  Research and Optimization Group (ERGO) </a> under the supervision
of <a href="https://richtarik.org/index.html"> Dr. Peter Richtarik</a>.
Prior to that, I spent 4 beautiful years in Athens as undergraduate
student in the Department of Mathematics at <a
  href="http://en.uoa.gr/"> National and Kapodistrian University of
  Athens</a> and 1 year as postgraduate student at Imperial College
London where I obtained an <a
href="http://www.imperial.ac.uk/computing/prospective-students/courses/pg/specialist-degrees/cms/">MSc
in      Computing (Computational Management Science)</a>. <br>
<br>
During fall of 2018, I was a research intern at <a
  href="https://research.fb.com/category/facebook-ai-research/">
  Facebook AI Research</a> in Montreal, Canada. I was working mainly
with <a href="https://research.fb.com/people/rabbat-mike/"> Dr.
  Mike Rabbat</a> on topics related to Distributed Non-Convex
Optimization Algorithms and Deep Learning. <br>
<br>
My research interests include (but are not limited to): <br>
Large Scale Optimization, Machine Learning, Randomized numerical
linear algebra, Convex Analysis, Randomized and Distributed
Algorithms. <br>
<br>
For more details please feel free to look my <a
  href="Resources/NicolasLoizouCV.pdf">CV (updated September 2020)</a>. <br>
<br>

<h2>Selected Recent News</h2>
<hr>
(For the full list of news please check the <a href="./news.html"> News </a> tab.)

    <ul>
      <li>
        <p> 29 Apr 2021: Our paper <a href="https://arxiv.org/abs/1905.08645">"Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols"</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://ieeeittrans.ee.technion.ac.il/"> IEEE Transactions on Information Theory. </a> </p>
      </li>
      <li>
        <p> 22 Jan 2021: Two papers were accepted to <b>AISTATS 2021</b> (24th International Conference on
Artificial Intelligence and Statistics):</p>
        <p><a href="https://arxiv.org/abs/2002.10542">Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence </a><br>
          joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien. </p>
      <p><a href="https://arxiv.org/abs/2006.10311">SGD for structured nonconvex functions: Learning rates, minibatching and interpolation </a> <br>
          joint work with Robert M. Gower and Othmane Sebbouh.</p>
      </li>
      <li>
        <p> 16 Oct 2020: I am delighted to be selected as the runner-up of the <a href="https://www.theorsociety.com/membership/awards-medals-and-scholarships/the-doctoral-award/"> OR Society’s Doctoral Award </a> for 2019.
This is an award for the "Most Distinguished Body of Research leading to the Award of a Doctorate in the field of Operational Research" in the United Kingdom. </li>
      <li>
        <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1903.07971">"Convergence Analysis of Inexact Randomized Iterative Methods"</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://www.siam.org/publications/journals/siam-journal-on-scientific-computing-sisc">SIAM Journal on Scientific Computing</a> (<b>SISC</b>) </p>
      </li>
      <li>
        <p> 19 Aug 2020: Our paper <a href="https://arxiv.org/abs/1712.09677">"Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods
  "</a>, joint work with Peter Richtarik,
          was accepted to
  <a href="https://www.springer.com/journal/10589">Computational Optimization and Applications</a> (<b>COAP</b>) </p>
      </li>
      <li>
        <p> 20 Jun 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2006.11573"> Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization
  </a>, joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower and Peter Richtárik. </p>
      </li>
      <li>
        <p> 18 Jun 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2006.10311"> SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation</a>, joint work with Robert M. Gower and Othmane Sebbouh. </p>
      </li>
      <li>
        <p> 01 Jun 2020: Two papers were accepted to <b>ICML 2020</b> (37th
          International Conference on Machine Learning):</p>
        <p><a href="https://arxiv.org/abs/2003.10422">A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates </a><br>
          joint work with Anastasia Koloskova, Sadra Boreiri, Martin
          Jaggi and Sebastian U. Stich. </p>
      <p><a href="https://arxiv.org/abs/2007.04202">Stochastic Hamiltonian Gradient Methods for Smooth
      Games </a> <br>
          joint work with Hugo Berard, Alexia Jolicoeur-Martineau,
          Pascal Vincent, Simon Lacoste-Julien and Ioannis
          Mitliagkas.</p>
      </li>
        <p> 23 Mar 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2003.10422"> A Unified
            Theory of Decentralized SGD with Changing Topology and
            Local Updates</a>, joint work with Anastasia Koloskova,
          Sadra Boreiri, Martin Jaggi and Sebastian U. Stich. </p>
      </li>
      <li>
        <p> 24 Feb 2020: <b>New Paper out:</b> <a
            href="https://arxiv.org/abs/2002.10542"> Stochastic
            Polyak Step-size for SGD: An Adaptive Learning Rate for
            Fast Convergence</a>, joint work with Sharan Vaswani,
          Issam Laradji and Simon Lacoste-Julien. </p>
      </li>
      <li>
        <p> 19 December 2019: I am delighted to be awarded the <a
            href="https://ivado.ca/en/ivado-scholarships/postdoctoral-scholarships/">
            IVADO Fellow Postdoctoral Scholarship </a>. <br>
          For more details on The Institute for Data Valorisation
          (IVADO) and its mission check out the <a
            href="https://ivado.ca/en/about-us/"> IVADO's website </a>
        </p>
      </li>
    </ul>
</div>
</div>
</div>

 </body>
</html>
